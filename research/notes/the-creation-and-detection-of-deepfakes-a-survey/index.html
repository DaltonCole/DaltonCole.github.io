
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">

    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-171431390-1"></script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-SPY7S5B91F"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-171431390-1');
    </script>

    <!-- Base File Includes -->
    
    <!-- LaTeX input. Use \(...\) for inline mathematics and $$...$$ or \[...\] for block equations -->
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


    <!-- Includes -->
    
    

    <!-- Title -->
    <title>
            DRC
        
    - Research Notes

    </title>
    <!-- ### Bootstrap Files ### -->
    <!-- Fit device screen size -->
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
    <!-- ### END BOOTSTRAP FILES ### -->
	
	<!-- Font Awesome 4 -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <!-- Load Custom Bootstrap CSS -->
    <link rel="stylesheet" type="text/css" href="/static/home/bootstrap.css">

    <!-- Favicon -->
    <link rel="shortcut icon" type="image/png" href="/static/home/images/favicon.ico">

    <!-- Hide/Reveal Navbar Upon Scrolling -->
    <script src="/static/home/js/navbar_show.js" async></script>
    <link rel="stylesheet" type="text/css" href="/static/home/css/navbar.css">

    <!-- Javascript to run after the page has loaded -->
    <script src="/static/home/js/on_page_load.js" defer></script>
    
</head>

<!-- Body -->
<body class='bg-light'>
    <!-- Navigation Bar -->
    <header class="nav-down">
        <nav class="navbar py-0 navbar-expand-md navbar-dark bg-dark">
    <!-- Website Brand Icon -->
    <a class="navbar-brand" href="/">DRC</a>
    <!-- Menu Button when screen is small -->
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <!-- Navigation Buttons -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav mr-auto">
            <!-- Home -->
            <li class="nav-item dropdown">
                <!-- Screen Reader -->
                
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/">Home</a>
                </div>
            </li>
            <!-- Experience -->
            <li class="nav-item dropdown">
                <!-- Screen Reader -->
                
                <!-- Experience Types -->
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/resume/">Experience</a>
                    <button type="button" class="btn btn-dark px-1 dropdown-toggle dropdown-toggle-split" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                        <span class="sr-only">Toggle Dropdown</span>
                    </button>
                    <div class="dropdown-menu py-0 bg-secondary">
                        <a class="dropdown-item" href="/resume/">&#8226; Resume</a>
                        <a class="dropdown-item text-white" href="/work-experience/">&#8226; Work Experience</a>
                        <!-- <a class="dropdown-item text-white" href="/computer-skills/">&#8226; Computer Skills</a> -->
                    </div>
                </div>
            </li>
 
            <!-- Blog Posts -->
<!--            <li class="nav-item dropdown">
                
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/blog/">Blog</a>
                    <button type="button" class="btn btn-dark px-1 dropdown-toggle dropdown-toggle-split" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                        <span class="sr-only">Toggle Dropdown</span>
                    </button>
                    <div class="dropdown-menu py-0 bg-secondary">
                        <a class="dropdown-item" href="/blog/project/">&#8226; Projects</a>
                        <a class="dropdown-item" href="/blog/tutorial/">&#8226; Tutorials</a>
                        <a class="dropdown-item" href="#">&#8226; Other</a>
                    </div>
                </div>
            </li>
-->        
            <!-- Research -->
            <li class="nav-item dropdown">
                <!-- Screen Reader -->
                
                <!-- Experience Types -->
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/research/">Research</a>
                    <button type="button" class="btn btn-dark px-1 dropdown-toggle dropdown-toggle-split" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                        <span class="sr-only">Toggle Dropdown</span>
                    </button>
                    <div class="dropdown-menu active py-0 bg-secondary">
                        <a class="dropdown-item" href="/research/papers/">&#8226; Papers</a>
                        <a class="dropdown-item" href="/research/projects/">&#8226; Projects</a>
                        <a class="dropdown-item" href="/research/notes/">&#8226; Notes</a>
                    </div>
                </div>
            </li>
            <!-- Learn -->
            <li class="nav-item dropdown">
                
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/learn/">Learn</a>
                </div>
            </li>
            <!-- Projects -->
            <li class="nav-item dropdown">
                
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/projects/">Projects</a>
                </div>
            </li>
            <!-- Write-Ups -->
            <li class="nav-item dropdown">
                
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/writeups/kattis/">Write-Ups</a>
                </div>
            </li>
            <!-- About -->
            <li class="nav-item dropdown">
                <!-- Screen Reader -->
                
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/about/">About</a>
                </div>
            </li>
            <!-- Contact -->
            <li class="nav-item dropdown">
                <!-- Screen Reader -->
                
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/contact/">Contact</a>
                </div>
            </li>
        </ul>
        <!-- Search  bar -->
        <form class="form-inline my-1" action="/search/">
            <input class="form-control mr-0" style="font-size:14px; width:85%;" type="search" placeholder="Search" aria-label="Search" name="q" autocomplete="on">
            <button type="submit" style="width:15%; background-color: Transparent; border-color: Transparent;">
                <i class="fa fa-search" style="font-size:18px; color:#FFFFFF;"></i>
            </button>
        </form>
    </div>
</nav>

    </header>

    <!-- Content -->
    <div style="margin-top:60px;">
        <div class="container rounded m-4 mx-auto p-4 bg-white shadow-lg">
            
    <p style="float:left;"><a href="/research/notes/">Back</a></p>
    <p style="float:right;">Disclaimer: These are my personal notes on this paper. I am in no way related to this paper. All credits go towards the authors.</p><br/><br/>
    <h1>The Creation and Detection of Deepfakes: A Survey</h1>
    <p>
        May 12, 2020 - 
        <a href="https://arxiv.org/abs/2004.11138" target='_blank'>Paper Link</a> - 
        Tags: Deepfake, Detection, Survey
    </p>
    
<h4>Notes</h4>
<ul>
    <li><b>Overview and Attack Models</b></li>
    <ul>
        <li>Four categories in human visuals: reenactment, replacement, editing, and synthesis. See figure 3 for examples.</li>
        <ul>
            <li>Reenactment: Target mimics source. Target has the same facial expressions as the source.</li>
            <li>Replacement: Swapping or transferring the target and source. Snapchat does this with their swap filter. This is the traditional idea of a deepfake.</li>
            <li>Editing: Photoshop.</li>
            <li>Synthesis: Generate a new face (like realistic automation). Has no target as a basis.</li>
        </ul>
    </ul>
    <li><b>Technical Background</b></li>
    <ul>
        <li>Figure 4 shows some example GAN (Generative Adversarial Network) examples.</li>
        <ul>
            <li>Encoder Decoder: The encoder takes as input a sample and transforms it into an embedding in "latent space". The embedding is then feed into the decoder to result in the original image.</li>
            <li>GAN: A GAN consists of a decoder and a discriminator. The decoder accepts as input a generated embedding and outputs a sample. The discriminator outputs if the sample was generated or a real sample. The encoder-decoder and discriminator has to be trained one after another, readily. Equation 2 and 3 show their loss functions.</li>
            <li>pix2pix: The generator tries to generate an image of the same context as something else. For example, make a painting look like a Picasso.</li>
            <li>Cycle GAN: Example - Two GANs. One makes an apple look like an orange and the other makes an orange look like an apple.</li>
        </ul>
        <li>Recurrent Neural Network (RNN): NN that can handle sequential and variable length data. Often used to handle audio and sometimes video data. Long short-term memory (LSTM) and Gate Recurrent Units (GRU) are used with RNNs</li>
        <li><b>Feature Representations</b></li>
        <ul>
            <li>Deep fakes often use an "intermediate representation to capture and sometimes manipulate the source and target's facial structure, pose, and expression."</li>
            <li>Useful to mark facial landmarks using Open CV.</li>
            <li>For speech, the Mel-Cepstral Coefficient is measured to capture the dominant voice frequencies.</li>
        </ul>
        <li>Figure 5 gives an overview of how a deepfake works in terms of face detection and blending (no NN).</li>
        <li>There are six approaches to derive an image</li>
        <ol>
            <li>Have a NN work directly on the image and perform the mapping itself</li>
            <li>"Train an ED network to disentangle the identity from the expression, and then modify/swap the encodings of the target before passing it through the decoder."</li>
            <li>"Add an additional encoding (e.g., AU or embedding) before passing it to the decoder."</li>
            <li>"Convert the intermediate face/body representation to the desired identity/expression before generation (e.g., transform the boundaries with a secondary network or render a 3D model of the target with the desired expression)."</li>
            <li>"Use the optimal flow field from subsequent frames in a source video to drive the generator."</li>
            <li>"Create a composite of the original content (hair, scene, etc) with a combination of the 3D rendering, warped image, or generated content, and pass the composite through another network (such as pix2pix) to refine the realism."</li>
        </ol>
        <li><b>Challenges in Creating Deepfakes</b></li>
        <ul>
            <li>Generalization - Many samples are required. Researchers try to minimize the number of training samples required</li>
            <li>Paired Training - Give the desired output for each input. Very laborious. Ways to avoid this issue: use frames from the same video as the target, use an unpaired network such as Cycle-GAN, or utilize the encodings of an encoder-decoder network</li>
            <li>Identity Leakage - Original identity is partially shown. Solutions: attention mechanisms, few-shot learning, disentanglement, boundary conversions, and AdaIN or skip connections to carry the relevant information to the generator</li>
            <li>Occlusions - When part of the source or target is obstructed by something (hand, hair, etc.)</li>
            <li>Temporal Coherence - Video artifacts, ex flickering or jittering. Since deepfakes generally work on a frame-by-frame basis, there is generally no context of preceding frames. Solutions: provide context to the generator and discriminator, implement temporal coherence losses, or use RNNs.</li>
        </ul>
        <li>Table 1 is a BIG table of Reenactment models (both body and face).</li>
    </ul>
    <li><b>Reenactment</b></li>
    <ul>
        <li>
            <b><i><u>Figures 6, 7, and 8</u></i></b> are a beautiful detailed view on how A LOT of networks work. <b>GREAT references</b>.
        </li>
        <li>Expression Reenactment</li>
        <ul>
            <li>One-to-One (Identity to Identity): <a href="https://arxiv.org/pdf/1710.06090.pdf" target="_blank">Xu et al.</a> used a CycleGAN for facial reenactment, without the need for data pairing. To avoid artifacts, both the source and target had similar distributions (poses and expressions).</li>
            <li>Many-to-One (Many Identities to a Single Identity): <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Bao_CVAE-GAN_Fine-Grained_Image_ICCV_2017_paper.pdf" target="_blank">Bao et al.</a> used a CVAE-GAN. </li>
            <li>Many-to-Many (Multiple IDs to Multiple IDs): 
                <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Olszewski_Realistic_Dynamic_Facial_ICCV_2017_paper.pdf" target="_blank">Conditional GAN</a>, 
                <a href="https://ieeexplore.ieee.org/abstract/document/8273626?casa_token=xYje0QkcKygAAAAA:XnGAWwi1ZDixTyGGnhkO3pbLkqXS193OdUhzx4PbogYxHPs2367ou5HnEgRT261ylYOAuErOKAPg" target="_blank">Zhou et al.,</a>
                <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf" target="_blank">StarGAN</a>,
                <a href="https://arxiv.org/pdf/1803.07716.pdf" target="_blank">GATH</a>
            </li>
        </ul>
        <li>Facial Boundary Conversion: <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Wayne_Wu_Learning_to_Reenact_ECCV_2018_paper.pdf" target="_blank">Wu et al.</a> used "ReenactGAN" a CycleGAN to transform the boundary of the source to the target's face, then applied a pix2pix-like generator.</li>
        <li>Temporal GANs</li>
        <ul>
            <li><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Tulyakov_MoCoGAN_Decomposing_Motion_CVPR_2018_paper.pdf", target="_blank">MoCoGAN</a>: a temoral GAN which generates videos while disentangling the motion and content (objects) in the process. Two discriminators are used, one for realism (per frame) and one for temporal coherence (last T frames)</li>
            <li><a href="https://arxiv.org/pdf/1808.06601.pdf" target="_blank">Want et al.</a> used Vid2Vid instead of RNNs, which is similar to pix2pix, but for videos. Vid2Vid considers the last N frames from the source and generator.</li>
            <li><a href="https://dl.acm.org/doi/pdf/10.1145/3197517.3201283" target="_blank">Kim et al.</a> used a GAN that does complete facial reenactment, including gazing, blinking, etc.</li>
        </ul>
    </ul>
    <ul>
        <li><b>CounterMeasures</b></li>
        <ul>
            <li>Table 3 summarizes deepfake detection models (on page 27).</li>
            <li>Detection</li>
            <ul>
                <li>Artifact-Based Detection (Most Popular)</li>
                <li>Generic Classifiers</li>
            </ul>
            <li>Prevention</li>
            <ul>
                <li>Have a global file system over Etherium using smart contracts to keep track of content.</li>
                <li>Add crafted noise perturbation to prevent deepfake technologies from locating a proper face.</li>
            </ul>
        </ul>
    </ul>
</ul>
<p>
    Citation: Mirsky, Yisroel, and Wenke Lee. "The Creation and Detection of Deepfakes: A Survey." arXiv preprint arXiv:2004.11138 (2020).
</p>


        </div>
    </div>

    <!-- Footer -->
    <footer class="small text-center text-muted">
        <!-- Social Links -->
		<p>
            <a href="https://github.com/DaltonCole" target="_blank" class="fa fa-github" style="font-size:28px;color:black;"></a>
            &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="https://www.linkedin.com/in/daltoncole1/" target="_blank" class="fa fa-linkedin" style="font-size:28px;color:#2867B2;"></a>
        </p>

        <!-- Copyright -->
        
        <p class="copyright"> Copyright &copy; 2020 
            -2024 
         | Dalton Russell Cole</p>

        <!-- Extra Footer Block -->
        
        
    </footer>
</body>
</html>
