
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">

    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-171431390-1"></script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-SPY7S5B91F"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-171431390-1');
    </script>

    <!-- Base File Includes -->
    
    <!-- LaTeX input. Use \(...\) for inline mathematics and $$...$$ or \[...\] for block equations -->
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


    <!-- Includes -->
    
    

    <!-- Title -->
    <title>
            DRC
        
    - Research Notes

    </title>
    <!-- ### Bootstrap Files ### -->
    <!-- Fit device screen size -->
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
    <!-- ### END BOOTSTRAP FILES ### -->
	
	<!-- Font Awesome 4 -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <!-- Load Custom Bootstrap CSS -->
    <link rel="stylesheet" type="text/css" href="/static/home/bootstrap.css">

    <!-- Favicon -->
    <link rel="shortcut icon" type="image/png" href="/static/home/images/favicon.ico">

    <!-- Hide/Reveal Navbar Upon Scrolling -->
    <script src="/static/home/js/navbar_show.js" async></script>
    <link rel="stylesheet" type="text/css" href="/static/home/css/navbar.css">

    <!-- Javascript to run after the page has loaded -->
    <script src="/static/home/js/on_page_load.js" defer></script>
    
</head>

<!-- Body -->
<body class='bg-light'>
    <!-- Navigation Bar -->
    <header class="nav-down">
        <nav class="navbar py-0 navbar-expand-md navbar-dark bg-dark">
    <!-- Website Brand Icon -->
    <a class="navbar-brand" href="/">DRC</a>
    <!-- Menu Button when screen is small -->
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <!-- Navigation Buttons -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav mr-auto">
            <!-- Home -->
            <li class="nav-item dropdown">
                <!-- Screen Reader -->
                
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/">Home</a>
                </div>
            </li>
            <!-- Experience -->
            <li class="nav-item dropdown">
                <!-- Screen Reader -->
                
                <!-- Experience Types -->
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/resume/">Experience</a>
                    <button type="button" class="btn btn-dark px-1 dropdown-toggle dropdown-toggle-split" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                        <span class="sr-only">Toggle Dropdown</span>
                    </button>
                    <div class="dropdown-menu py-0 bg-secondary">
                        <a class="dropdown-item" href="/resume/">&#8226; Resume</a>
                        <a class="dropdown-item text-white" href="/work-experience/">&#8226; Work Experience</a>
                        <!-- <a class="dropdown-item text-white" href="/computer-skills/">&#8226; Computer Skills</a> -->
                    </div>
                </div>
            </li>
 
            <!-- Blog Posts -->
<!--            <li class="nav-item dropdown">
                
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/blog/">Blog</a>
                    <button type="button" class="btn btn-dark px-1 dropdown-toggle dropdown-toggle-split" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                        <span class="sr-only">Toggle Dropdown</span>
                    </button>
                    <div class="dropdown-menu py-0 bg-secondary">
                        <a class="dropdown-item" href="/blog/project/">&#8226; Projects</a>
                        <a class="dropdown-item" href="/blog/tutorial/">&#8226; Tutorials</a>
                        <a class="dropdown-item" href="#">&#8226; Other</a>
                    </div>
                </div>
            </li>
-->        
            <!-- Research -->
            <li class="nav-item dropdown">
                <!-- Screen Reader -->
                
                <!-- Experience Types -->
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/research/">Research</a>
                    <button type="button" class="btn btn-dark px-1 dropdown-toggle dropdown-toggle-split" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                        <span class="sr-only">Toggle Dropdown</span>
                    </button>
                    <div class="dropdown-menu active py-0 bg-secondary">
                        <a class="dropdown-item" href="/research/papers/">&#8226; Papers</a>
                        <a class="dropdown-item" href="/research/projects/">&#8226; Projects</a>
                        <a class="dropdown-item" href="/research/notes/">&#8226; Notes</a>
                    </div>
                </div>
            </li>
            <!-- Learn -->
            <li class="nav-item dropdown">
                
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/learn/">Learn</a>
                </div>
            </li>
            <!-- Projects -->
            <li class="nav-item dropdown">
                
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/projects/">Projects</a>
                </div>
            </li>
            <!-- Write-Ups -->
            <li class="nav-item dropdown">
                
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/writeups/kattis/">Write-Ups</a>
                </div>
            </li>
            <!-- About -->
            <li class="nav-item dropdown">
                <!-- Screen Reader -->
                
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/about/">About</a>
                </div>
            </li>
            <!-- Contact -->
            <li class="nav-item dropdown">
                <!-- Screen Reader -->
                
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/contact/">Contact</a>
                </div>
            </li>
        </ul>
        <!-- Search  bar -->
        <form class="form-inline my-1" action="/search/">
            <input class="form-control mr-0" style="font-size:14px; width:85%;" type="search" placeholder="Search" aria-label="Search" name="q" autocomplete="on">
            <button type="submit" style="width:15%; background-color: Transparent; border-color: Transparent;">
                <i class="fa fa-search" style="font-size:18px; color:#FFFFFF;"></i>
            </button>
        </form>
    </div>
</nav>

    </header>

    <!-- Content -->
    <div style="margin-top:60px;">
        <div class="container rounded m-4 mx-auto p-4 bg-white shadow-lg">
            
    <p style="float:left;"><a href="/research/notes/">Back</a></p>
    <p style="float:right;">Disclaimer: These are my personal notes on this paper. I am in no way related to this paper. All credits go towards the authors.</p><br/><br/>
    <h1>FaceForensics++: Learning to Detect Manipulated Facial Images</h1>
    <p>
        Aug. 26, 2019 - 
        <a href="https://arxiv.org/pdf/1901.08971.pdf" target='_blank'>Paper Link</a> - 
        Tags: Dataset, Deepfake, Detection, Facial-Reenactment, Survey
    </p>
    
    <h4>Summary</h4>
        Created a large video database consisting of 1000 videos each manipulated (automatically) via Deepfakes, Face2Face, FaceSwap, and NeuralTextures. Used five different state of the art detection methods to see how well they were able to detect real vs fake images (frames). XceptionNet outperformed the other methods. Each method did worse on low quality images and best on raw quality images (i.e. compression is hard).
    <h4>Notes</h4>
        <ul>
            <li>Made a dataset consisting of 1000 video sequences that have been manipulated with via Deepfakes, Face2Face, FaceSwap, and NeuralTextures. <a href="https://github.com/ondyari/FaceForensics" target="_blank">LINK</a></li>
            <li>There are currently two types of facial manipulation methods: Facial expression manipulation and facial identity manipulation. Figure 2 gives a good example.</li>
            <ul>
                <li>Identity Swap: The face from one person is transposed onto another person</li>
                <ul>
                    <li><b>FaceSwap</b></li>
                    <ul>
                        <li>"Graphics-based approach to transfer the face region from a source video to a target video"</li>
                        <li>Finds facial landmarks &rarr; Extract face &rarr; Back projected to a target image by minimizing landmark distances &rarr; Blend image and apply color correction </li>
                    </ul>
                    <li><b>DeepFakes</b></li>
                    <ul>
                        <li>FakeApp and <a href="https://github.com/deepfakes/faceswap" target="_blank">faceswap</a> (DeepFake Method) are public implementations</li>
                        <li>Uses neural networks: single encoder and two decoders</li>
                        <li>Face detector used to crop and align images. <a href="https://www.cs.jhu.edu/~misha/Fall07/Papers/Perez03.pdf" target="_blank">Poisson</a> image editing is used to blend the image</li>
                    </ul>
                </ul>
                <li>Facial Reenactment: The facial expression from one person is transposed onto another. The original identity if maintained, only the expression changes.</li>
                <ul>
                    <li><b><a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Thies_Face2Face_Real-Time_Face_CVPR_2016_paper.pdf" href="_blank">Face2Face</a></b></li>
                    <ul>
                        <li>A "dense reconstruction" of the face is generated via following the face through multiple frames of a video. This is "used to re-synthesize the face under different illumination and expressions"</li>
                    </ul>
                    <li><b><a href="https://dl.acm.org/doi/pdf/10.1145/3306346.3323035" target="_blank">NeuralTextures</a></b></li>
                    <ul>
                        <li>"Uses original video data to learn a neural texture of the target person"</li>
                        <li>Uses a unique model for every manipulation. Proves the most difficult for the below detectors.</li>
                    </ul>
                </ul>
            </ul>
            <li>Forgery Detection - A per-frame binary classification problem ("Real" or "Fake")</li>
            <ul>
                <li>Human</li>
                <ul>
                    <li>Figure 4 highlights their results when humans classify images as real or fake. Facial reenactment was really hard to detect (around 50% accuracy, i.e. random guessing). Identity swap was much easier to detect with generally &gt;70% accuracy. It was generally harder to tell the difference between real and fake with lower quality images</li>
                </ul>
                <li>Automated</li>
                <ul>
                    <li>Figure 6 summarizes the accuracy results</li>
                    <li>Steganalysis Features</li>
                    <ul>
                        <li>Based on a method by <a href="https://ieeexplore.ieee.org/document/6197267" target="_blank">Fridrich et al.</a> which won the first <a href="https://ieeexplore.ieee.org/abstract/document/7026072?casa_token=S6Vbh_TBdrYAAAAA:tv1JkXr-jGguT6IJ9Txg_iVOlW8SPhFU1lmsoRXcfU353nsBcN7GaZdzePFBlG1k3H-24Y4CsCor" target="_blank">IEEE Image Forensic Challenge</a>. Uses 162 features derived from a 128x128 central crop-out image of the face, which is then feed into a SVM</li>
                        <li>Preforms really well on raw image, but struggles with compressed images</li>
                    </ul>
                    <li><a href="https://dl.acm.org/doi/pdf/10.1145/3082031.3083247" target="_blank">Cozzolino et al.</a></li>
                    <ul>
                        <li>Used the same features from the "Steganalysis Features" classifier, but used a CNN-based network instead of a SVM.</li>
                        <li>Preforms better than the SVM Steganalysis features method, but still stugles with low quality videos</li>
                    </ul>
                    <li><a href="https://dl.acm.org/doi/pdf/10.1145/2909827.2930786" target="_blank">Bayar and Stamm</a></li>
                    <ul>
                        <li>CNN based approach that uses constrained convolutional layers</li>
                        <li>Better than the previous two methods. Still struggles with low quality images</li>
                    </ul>
                    <li><a href="https://ieeexplore.ieee.org/abstract/document/8267647?casa_token=Zme-MdYCwHcAAAAA:XuWqdTEPb8j3QR-UaBkDmmJqWQjg26tGkdPt8hYr004xa5qn2i82H2v7bfgK9cwOzkE5kcdXSAUm" target="_blank">Rehmouni et al.</a></li>
                    <ul>
                        <li>CNN based approach. Computes some statistics.</li>
                        <li>Roughly the same results as Cozzolino et al.'s method</li>
                    </ul>
                    <li><a href="https://ieeexplore.ieee.org/abstract/document/8630761?casa_token=7BGihSwA2KMAAAAA:dA17qHWCAisibseWERfeuZkZ8eBoBSjk_Ggec6BhZ4swyvAmjb5SM18rV1S4MkRN_4gXu0-4Y1C4" target="_blank">MesoInception-4</a></li>
                    <ul>
                        <li>CNN based approach inspired by <a href="https://arxiv.org/abs/1602.07261" target="_blank">InceptionNet</a></li>
                        <li>Has two inception modules and two classical convolution layers</li>
                        <li>Better results than the previous 4 methods (generally)</li>
                    </ul>
                    <li><a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf" target="_blank">XceptionNet</a></li>
                    <ul>
                        <li>Traditional CNN trained on ImageNet. Based on separate convolutions with residual connections. They re-fitted this network for their purposes by transferring most of the network and replacing the final connected layer with two outputs.</li>
                        <li>Best results. Preforms well with low quality images.</li>
                    </ul>
                    <li>Figure 8 shows how important a large dataset is.</li>
                </ul>
            </ul>
        </ul>

    <h4>Interesting References</h4>
        <ul>
            <li><a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Thies_Face2Face_Real-Time_Face_CVPR_2016_paper.pdf" href="_blank">Face2Face</a> uses a state-of-the-art face tracking method. Good for extracting the face region in an image.</li>
        </ul>
    <p>
        Citation: Rossler, Andreas, et al. "Faceforensics++: Learning to detect manipulated facial images." Proceedings of the IEEE International Conference on Computer Vision. 2019.
    </p>


        </div>
    </div>

    <!-- Footer -->
    <footer class="small text-center text-muted">
        <!-- Social Links -->
		<p>
            <a href="https://github.com/DaltonCole" target="_blank" class="fa fa-github" style="font-size:28px;color:black;"></a>
            &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="https://www.linkedin.com/in/daltoncole1/" target="_blank" class="fa fa-linkedin" style="font-size:28px;color:#2867B2;"></a>
        </p>

        <!-- Copyright -->
        
        <p class="copyright"> Copyright &copy; 2020 
            -2024 
         | Dalton Russell Cole</p>

        <!-- Extra Footer Block -->
        
        
    </footer>
</body>
</html>
