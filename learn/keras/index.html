
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">

    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-171431390-1"></script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-SPY7S5B91F"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-171431390-1');
    </script>

    <!-- Base File Includes -->
    
    

    <!-- Includes -->
    
    <!-- Code highlighting
        To Use: <pre><code class="language-python">print('Hello World')</code></pre>
    -->
    <link rel="stylesheet" href="/static/home/packages/highlightjs/dracula.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <!-- LaTeX input. Use \(...\) for inline mathematics and $$...$$ or \[...\] for block equations -->
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


    <!-- Title -->
    <title>
            DRC
        
    - Learn Keras

    </title>
    <!-- ### Bootstrap Files ### -->
    <!-- Fit device screen size -->
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
    <!-- ### END BOOTSTRAP FILES ### -->
	
	<!-- Font Awesome 4 -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <!-- Load Custom Bootstrap CSS -->
    <link rel="stylesheet" type="text/css" href="/static/home/bootstrap.css">

    <!-- Favicon -->
    <link rel="shortcut icon" type="image/png" href="/static/home/images/favicon.ico">

    <!-- Hide/Reveal Navbar Upon Scrolling -->
    <script src="/static/home/js/navbar_show.js" async></script>
    <link rel="stylesheet" type="text/css" href="/static/home/css/navbar.css">

    <!-- Javascript to run after the page has loaded -->
    <script src="/static/home/js/on_page_load.js" defer></script>
    
</head>

<!-- Body -->
<body class='bg-light'>
    <!-- Navigation Bar -->
    <header class="nav-down">
        <nav class="navbar py-0 navbar-expand-md navbar-dark bg-dark">
    <!-- Website Brand Icon -->
    <a class="navbar-brand" href="/">DRC</a>
    <!-- Menu Button when screen is small -->
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <!-- Navigation Buttons -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav mr-auto">
            <!-- Home -->
            <li class="nav-item dropdown">
                <!-- Screen Reader -->
                
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/">Home</a>
                </div>
            </li>
            <!-- Experience -->
            <li class="nav-item dropdown">
                <!-- Screen Reader -->
                
                <!-- Experience Types -->
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/resume/">Experience</a>
                    <button type="button" class="btn btn-dark px-1 dropdown-toggle dropdown-toggle-split" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                        <span class="sr-only">Toggle Dropdown</span>
                    </button>
                    <div class="dropdown-menu py-0 bg-secondary">
                        <a class="dropdown-item" href="/resume/">&#8226; Resume</a>
                        <a class="dropdown-item text-white" href="/work-experience/">&#8226; Work Experience</a>
                        <!-- <a class="dropdown-item text-white" href="/computer-skills/">&#8226; Computer Skills</a> -->
                    </div>
                </div>
            </li>
 
            <!-- Blog Posts -->
<!--            <li class="nav-item dropdown">
                
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/blog/">Blog</a>
                    <button type="button" class="btn btn-dark px-1 dropdown-toggle dropdown-toggle-split" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                        <span class="sr-only">Toggle Dropdown</span>
                    </button>
                    <div class="dropdown-menu py-0 bg-secondary">
                        <a class="dropdown-item" href="/blog/project/">&#8226; Projects</a>
                        <a class="dropdown-item" href="/blog/tutorial/">&#8226; Tutorials</a>
                        <a class="dropdown-item" href="#">&#8226; Other</a>
                    </div>
                </div>
            </li>
-->        
            <!-- Research -->
            <li class="nav-item dropdown">
                <!-- Screen Reader -->
                
                <!-- Experience Types -->
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/research/">Research</a>
                    <button type="button" class="btn btn-dark px-1 dropdown-toggle dropdown-toggle-split" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                        <span class="sr-only">Toggle Dropdown</span>
                    </button>
                    <div class="dropdown-menu active py-0 bg-secondary">
                        <a class="dropdown-item" href="/research/papers/">&#8226; Papers</a>
                        <a class="dropdown-item" href="/research/projects/">&#8226; Projects</a>
                        <a class="dropdown-item" href="/research/notes/">&#8226; Notes</a>
                    </div>
                </div>
            </li>
            <!-- Learn -->
            <li class="nav-item dropdown">
                
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/learn/">Learn</a>
                </div>
            </li>
            <!-- Projects -->
            <li class="nav-item dropdown">
                
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/projects/">Projects</a>
                </div>
            </li>
            <!-- Write-Ups -->
            <li class="nav-item dropdown">
                
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/writeups/kattis/">Write-Ups</a>
                </div>
            </li>
            <!-- About -->
            <li class="nav-item dropdown">
                <!-- Screen Reader -->
                
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/about/">About</a>
                </div>
            </li>
            <!-- Contact -->
            <li class="nav-item dropdown">
                <!-- Screen Reader -->
                
                <div class="btn-group ">
                    <a class="nav-link btn btn-dark pr-1" href="/contact/">Contact</a>
                </div>
            </li>
        </ul>
        <!-- Search  bar -->
        <form class="form-inline my-1" action="/search/">
            <input class="form-control mr-0" style="font-size:14px; width:85%;" type="search" placeholder="Search" aria-label="Search" name="q" autocomplete="on">
            <button type="submit" style="width:15%; background-color: Transparent; border-color: Transparent;">
                <i class="fa fa-search" style="font-size:18px; color:#FFFFFF;"></i>
            </button>
        </form>
    </div>
</nav>

    </header>

    <!-- Content -->
    <div style="margin-top:60px;">
        <div class="container rounded m-4 mx-auto p-4 bg-white shadow-lg">
            
<div class="row">
    <!-- Scroll Bar -->
    <div class="d-none d-xl-block col-xl-2 overflow-auto" style="position:relative; overflow-y:scroll; height:80vh">
        <div id="side-navbar" class="list-group flex-xl-column flex-row">
            <center>
                <a class="list-group-item list-group-item-action" href="#keras">
                    Keras
                </a>
            </center>
            
                <a class="list-group-item list-group-item-action" href="#callbacks">
                    Callbacks
                </a>
            
                <a class="list-group-item list-group-item-action" href="#layers">
                    Layers
                </a>
            
                <a class="list-group-item list-group-item-action" href="#models">
                    Models
                </a>
            
        </div>
    </div>
    <!-- Content -->
    <div class="col" data-spy="scroll" data-target="#side-navbar" class="scrollspy" style="position:relative; overflow-y:scroll; height:80vh">
        <!-- Theme Information -->
        <h1 id='keras' class='text-center'>Keras</h1>
        <p><a href="https://keras.io/" target="_blank">Keras</a> is a high-level deep learning API framework for machine learning platforms such as <a href="https://www.tensorflow.org/" target="_blank">TensorFlow</a>. </p>

<p><a href="https://keras.io/api/" target="_blank">Docs</a>.</p>

        <!-- For each subject -->
        
            <!-- Subject header -->
            <h2 id="callbacks">
                <a href="/learn/keras/callbacks">Callbacks</a>
            </h2>
            
            <!-- For each topic -->
            
        
            <!-- Subject header -->
            <h2 id="layers">
                <a href="/learn/keras/layers">Layers</a>
            </h2>
            <p>A model is made up of multiple <a href="https://keras.io/api/layers/" target="_blank">layers</a>.<!-- A layer has at least three components, a layer type, shape, and activation function. Under-the-hood, a layer is just a set of weights --></p>

            <!-- For each topic -->
            
                <!-- Topic header -->
                <h4 id="layers-activation-functions">&bull; Activation Functions</h4>
                <!-- Include topic -->
                <p>The <a href="https://keras.io/api/layers/activations/" target="_blank">activation function</a> determines what is outputted by neurons of this layer. There are two ways to add an activation function to a layer. You can either add it via the <code>activation</code> argument on any layer or you can add the activation function as a layer.</p>

<ul>
    <li>Argument:</li>
</ul>

<pre class="rounded"><code class="python">model.add(keras.layers.Dense(32, activation='relu'))</code></pre>

<ul>
    <li>As a layer:</li>
</ul>

<pre class="rounded"><code class="python">model.add(keras.layers.Dense(32))
model.add(keras.layers.Activation('relu'))</code></pre>

<p>Built in activation functions:</p>

<ul>
    <li>relu</li>
    <ul>
        <li>The ReLU or rectified linear unit activation function: <code>max(x, 0)</code></li>
        <li>Is the generic activation function.</li>
        <li>To clip, you can set the max value using the <code>max_value</code> argument or the min value using the <code>threshold</code> argument.</li>
    </ul>
    <li>sigmoid</li>
    <ul>
        <li>\( \theta (x) = \frac{1}{1 + e^{-x}} \)</li>
        <li>Values are between 0 and 1</li>
        <li>S-Shaped</li>
        <li>Great for making the last layer output a probability.</li>
    </ul>
    <li>softmax</li>
    <ul>
        <li>Converts a real vector to a vector of categorical probabilities.</li>
        <li>Each output is the probability of that output. All outputs sum to 1.</li>
        <li>Generally used as the activation function for the <i>last layer</i>.</li>
    </ul>
    <li>softplus</li>
    <ul>
        <li>\( softplus(x) = log(e^x + 1) \)</li>
    </ul>
    <li>softsign</li>
    <ul>
        <li>\( softsign(x) = \frac{x}{\lvert x \rvert + 1} \)</li>
    </ul>
    <li>tanh</li>
    <ul>
        <li>Hyperbolic tangent. \( tanh(x) = \frac{sinh(x)}{cosh(x)} = \frac{e^x - e^{-x}}{e^x + e^{-x}} \)</li>
        <li>Outputs are between (-1, 1)</li>
        <li>S-Shaped</li>
        <li>Advantages: &bull;Negative numbers are perserved &bull;Zero inputs are mapped near zero</li>
    </ul>
    <li>selu</li>
    <ul>
        <li>The SELU or scaled exponential linear unit is related to the ReLU activation function and super related to the Leaky ReLU activation function.</li>
        <ul>
            <li><code>if x &ge; 0: return scale * x</code></li>
            <li><code>if x &le; 0: return scale * alpha * (exp(x) - 1)</code></li>
        </ul>
        <li>Unlike ReLU, SELU allows negative values, so these cells cannot die (all become 0).</li>
        <li>Compared to leaky ReLU, there is an exponential dip instead of a straight line for negative values.</li>
    </ul>
    <li>elu</li>
    <ul>
        <li>Exponential Linear Unit. SELU, but without the scale.</li>
    </ul>
    <li>exponential</li>
</ul>

<p>Advanced Activation Functions:</p>
<ul>
    <li>LeakyReLU</li>
    <ul>
        <li><code>tf.keras.layers.LeackyReLU(alpha=0.3)</code></li>
        <li>Like exponential linear unit, but negative values are a linear line with a slight slope.</li>
    </ul>
</ul>

<p>Pros and Cons of most activation function: <a href="https://mlfromscratch.com/activation-functions-explained/#/" target="_blank">HERE</a>.</p>

            
                <!-- Topic header -->
                <h4 id="layers-attention-layers">&bull; Attention Layers</h4>
                <!-- Include topic -->
                
            
                <!-- Topic header -->
                <h4 id="layers-base-layer-class">&bull; Base Layer Class</h4>
                <!-- Include topic -->
                <p>All layers inherit from the <a href="https://keras.io/api/layers/base_layer/" target="_blank">base layer</a> class.</p>

<pre class="rounded"><code class="python">tf.keras.layers.Layer(
    trainable=True, name=None, dtype=None, dynamic=False, **kwargs   
)</code></pre>

<ul>
    <li>Trainable: If false, the layer is <i>frozen</i> and will not be trained.</li>
    <li>Name: Name a layer. Can be accessed via <code>model.layers[0].name</code></li>
</ul>

            
                <!-- Topic header -->
                <h4 id="layers-convolutional-layers">&bull; Convolutional Layers</h4>
                <!-- Include topic -->
                <p>Convolutional layers are used with convolutional neural networks (CNNs)</p>

<h5>Conv*D</h5>
<p>There are three different convolutional layer dimensions:</p>

<ul>
    <li><a href="https://keras.io/api/layers/convolution_layers/convolution1d/" target="_blank">Conv1D</a>: Ex temporal convolutions</li>
    <li><a href="https://keras.io/api/layers/convolution_layers/convolution2d/" target="_blank">Conv2D</a>: Ex spatial convolutions over images</li>
    <li><a href="https://keras.io/api/layers/convolution_layers/convolution3d/" target="_blank">Conv3D</a>: Ex spatial convolutions over volumes</li>
</ul>

<pre class="rounded"><code class="python">tf.keras.layers.Conv2D(
    filters,
    kernel_size,
    strides=(1, 1),
    padding="valid",
    data_format=None,
    dilation_rate=(1, 1),
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer="glorot_uniform",
    bias_initializer="zeros",
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)</code></pre>

<ul>
    <li>Filters: Number of output filers</li>
    <li>Kernel Size: Height and width of the 2D convolutional window. Tuple of ints</li>
    <li>Strides: Specifies the strides (movement of the window) along the height and width.</li>
    <li>Padding:</li>
    <ul>
        <li>'valid': No padding. Output shape will be smaller than input shape</li>
        <li>'same': Adds padding. Output shape (height/width, not channels/filters) will be the same as the input</li>
    </ul>
    <li>data_format</li>
    <ul>
        <li>'channels_last' (default): Expects <code>(batch_size, height, width, channels)</code> as input</li>
        <li>'channels_first': Expects <code>(batch_size, channels, height, width)</code> as input</li>
    </ul>
</ul>

<p>Input Shape: <code>batch_shape + (channels, rows, cols)</code> if data format is channels first</p>
<p>Output Shape: <code>batch_shape + (filters, new_rows, new_cols)</code> if data format is channels first. Rows and columns may change due to padding.</p>


<h5>SeperableConv*D</h5>
<p>Similar to Conv*D layers, but channels are kept separate at first and then mixed at the end. The 2D version is similar to an Inception Block.</p>
<ul>
    <li><a href="https://keras.io/api/layers/convolution_layers/separable_convolution1d/" target="_blank">SeparableConv1D</a></li>
    <li><a href="https://keras.io/api/layers/convolution_layers/separable_convolution2d/" target="_blank">SeparableConv2D</a></li>
</ul>

<h5><a href="https://keras.io/api/layers/convolution_layers/depthwise_convolution2d/" target="_blank"></a>DepthwiseConv2D</h5>
<p>Performs the first half of SeperableConv2D, where channels are kept separate.</p>

<h5>Conv*DTranspose (Deconvolution)</h5>
<p>"Undoes" a convolutional layer. Is generally used to increase the dimensionality (rows and columns) while decreasing the channel number.</p>
<ul>
    <li><a href="https://keras.io/api/layers/convolution_layers/convolution2d_transpose/" target="_blank">Conv2DTranspose</a></li>
    <li><a href="https://keras.io/api/layers/convolution_layers/convolution3d_transpose/" target="_blank">Conv3DTranspose</a></li>
</ul>

<pre class="rounded"><code class="python">tf.keras.layers.Conv2DTranspose(
    filters,
    kernel_size,
    strides=(1, 1),
    padding="valid",
    output_padding=None,
    data_format=None,
    dilation_rate=(1, 1),
    activation=None,
    use_bias=True,
    kernel_initializer="glorot_uniform",
    bias_initializer="zeros",
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)</code></pre>


            
                <!-- Topic header -->
                <h4 id="layers-core-layers">&bull; Core Layers</h4>
                <!-- Include topic -->
                <p></p>

<h5><a href="https://keras.io/api/layers/core_layers/input/">Input</a></h5>
<p>Used to instantiate a keras tensor</p>
<pre class="rounded"><code class="python">tf.keras.Input(
    shape=None,
    batch_size=None,
    name=None,
    dtype=None,
    sparse=False,
    tensor=None,
    ragged=False,
    **kwargs
)</code></pre>
<ul>
    <li>Shape: Input shape, not including batch size. Should be a tuple of integers.</li>
</ul>

<h5><a href="https://keras.io/api/layers/core_layers/dense/" target=_blank>Dense</a></h5>
<p>The most common layer type. A layer that is completely connected to the previous layer.</p>
<pre class="rounded"><code class="python">tf.keras.layers.Dense(
    units,
    activation=None,
    use_bias=True,
    kernel_initializer="glorot_uniform",
    bias_initializer="zeros",
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)</code></pre>
<ul>
    <li>Units: The number of neurons</li>
</ul>

<h5><a href="https://keras.io/api/layers/core_layers/activation/" target="_blank">Activation</a></h5>
<p>Add an activation function to the previous layer.</p>
<pre class="rounded"><code class="python">tf.keras.layers.Activation(activation, **kwargs)</code></pre>

<h5><a href="https://keras.io/api/layers/core_layers/embedding/" target="_blank">Embedding</a></h5>
<p><a href="https://gdcoder.com/what-is-an-embedding-layer/" target="_blank">THIS</a> site does a great job of explaining what an embedding layer does. "an embedding learns tries to find the optimal mapping of each of the unique words to a vector of real numbers. The size of that vectors is equal to the output_dim". An embedding layer maps a vector that consists of a small sample of the vocabulary to a feature vector.</p>
<p>Must be the first layer of a model.</p>
<pre class="rounded"><code class="python">tf.keras.layers.Embedding(
    input_dim,
    output_dim,
    embeddings_initializer="uniform",
    embeddings_regularizer=None,
    activity_regularizer=None,
    embeddings_constraint=None,
    mask_zero=False,
    input_length=None,
    **kwargs
)</code></pre>
<ul>
    <li>Input Dim: Vocabulary size, number of possible unique words in an input vector.</li>
    <li>Output Dim: Dimension of the dense embedding (size of the feature vector for each unique word)</li>
    <li>Input Length: Use if the input is of a constant length. Required if using <code>Flatten</code> followed by <code>Dense</code> later on.</li>
</ul>

<h5><a href="https://keras.io/api/layers/core_layers/masking/" target="_blank">Masking</a></h5>
<p>Used primarily in RNNs. Skips timesteps. Good for skipping padding when using LSTM.</p>
<pre class="rounded"><code class="python">tf.keras.layers.Masking(mask_value=0.0, **kwargs)</code></pre>

<h5><a href="https://keras.io/api/layers/core_layers/lambda/" target="_blank">Lambda</a></h5>
<pre class="rounded"><code class="python">tf.keras.layers.Lambda(
    function, output_shape=None, mask=None, arguments=None, **kwargs
)</code></pre>
<ul>
    <li>Function: Lambda function</li>
</ul>

            
                <!-- Topic header -->
                <h4 id="layers-locally-connected-layers">&bull; Locally Connected Layers</h4>
                <!-- Include topic -->
                
            
                <!-- Topic header -->
                <h4 id="layers-merging-layers">&bull; Merging Layers</h4>
                <!-- Include topic -->
                
            
                <!-- Topic header -->
                <h4 id="layers-normalization-layers">&bull; Normalization Layers</h4>
                <!-- Include topic -->
                
            
                <!-- Topic header -->
                <h4 id="layers-pooling-layers">&bull; Pooling Layers</h4>
                <!-- Include topic -->
                <p>Pooling layers are used to downsample. They are generally used with convolutional layers to reduce the size of the feature space</p>

<h5>MaxPooling*D</h5>
<p>Max pooling uses passes the max value over a window to the next layer. There are three different pooling layer dimensions:</p>

<ul>
    <li><a href="https://keras.io/api/layers/pooling_layers/max_pooling1d/" target="_blank">Conv1D</a>: Ex: temporal data</li>
    <li><a href="https://keras.io/api/layers/pooling_layers/max_pooling2d/" target="_blank">Conv2D</a>: Ex spatial data (images)</li>
    <li><a href="https://keras.io/api/layers/pooling_layers/max_pooling3d/" target="_blank">Conv3D</a>: Ex 3D data (spatial or spatio-temporal)</li>
</ul>

<pre class="rounded"><code class="python">tf.keras.layers.MaxPooling2D(
    pool_size=(2, 2), 
    strides=None, 
    padding="valid", 
    data_format=None, 
    **kwargs
)</code></pre>

<ul>
    <li>Pool Size: Size of the window</li>
    <li>Strides: How far the window moves after each pooling step (int or tuple of ints)</li>
    <li>Padding:</li>
    <ul>
        <li>'valid': No padding. <code>output_shape = (input_shape - pool_size + 1) / strides)</code></li>
        <li>'same': Output will have the same height/width dimensions as input. <code>output_shape = input_shape / strides</code></li>
    </ul>
    <li>Data Format: 'channels_last' or 'channels_first'</li>
</ul>

<h5>AveragePooling*D</h5>
<p>Average pooling passes the average value over a window to the next layer. There are three different pooling layer dimensions:</p>

<ul>
    <li><a href="https://keras.io/api/layers/pooling_layers/average_pooling1d/" target="_blank">AveragePooling1D</a>: Ex: temporal data</li>
    <li><a href="https://keras.io/api/layers/pooling_layers/average_pooling2d/" target="_blank">AveragePooling2D</a>: Ex: spatial data (images)</li>
    <li><a href="https://keras.io/api/layers/pooling_layers/average_pooling3d/" target="_blank">AveragePooling3D</a>: Ex: 3D data (spatial or spatio-temporal)</li>
</ul>

<pre class="rounded"><code class="python">tf.keras.layers.AveragePooling2D(
    pool_size=(2, 2), strides=None, padding="valid", data_format=None, **kwargs
)</code></pre>

<p>Args same as MaxPooling</p>

<h5>Other</h5>
<p>There are also <a href="https://keras.io/api/layers/pooling_layers/global_max_pooling2d/" target="_blank">GlobalMaxPooling</a> and <a href="https://keras.io/api/layers/pooling_layers/global_average_pooling2d/" target="_blank">GlobalAveragePooling</a> varients that don't use a window, but the entire input.</p>

            
                <!-- Topic header -->
                <h4 id="layers-preprocessing-layers">&bull; Preprocessing Layers</h4>
                <!-- Include topic -->
                
            
                <!-- Topic header -->
                <h4 id="layers-recurrent-layers">&bull; Recurrent Layers</h4>
                <!-- Include topic -->
                
            
                <!-- Topic header -->
                <h4 id="layers-reshaping-layers">&bull; Reshaping Layers</h4>
                <!-- Include topic -->
                
            
                <!-- Topic header -->
                <h4 id="layers-weight-contraints">&bull; Weight Contraints</h4>
                <!-- Include topic -->
                <p><a href="https://keras.io/api/layers/constraints/" target="_blank">Constraints</a> can be added to the weights of a layer. For example, a constraint might not allow negative weights or a constraint might limit the norm of a layer.</p>

            
                <!-- Topic header -->
                <h4 id="layers-weight-initializers">&bull; Weight Initializers</h4>
                <!-- Include topic -->
                <p><a href="https://keras.io/api/layers/initializers/" target="_blank">Weight initalizers</a> initialize a layer's weights.</p>

            
                <!-- Topic header -->
                <h4 id="layers-weight-regularizers">&bull; Weight Regularizers</h4>
                <!-- Include topic -->
                <p><a href="https://keras.io/api/layers/regularizers/" target="_blank">Weight regularizers</a> penalizes certain aspects of a layer's parameters during optimization (training).</p>

<p>Three common regulaizers exist for most layer types:</p>
<ul>
    <li><code>kernel_regularizer</code>: Applies regularization function to the weights matrix</li>
    <li><code>bias_regularizer</code>: Applies regularization function to the bias</li>
    <li><code>activity_regularizer</code>: Applies regularization function to the output of the layer</li>
</ul>

<p>Good stackexchange <a href="https://stats.stackexchange.com/questions/383310/what-is-the-difference-between-kernel-bias-and-activity-regulizers-and-when-t" target="_blank">post</a> about the three regularizers.</p>

<p>There are three available regularizers:</p>
<ul>
    <li><code>tf.keras.regularizers.l1(l1=0.01)</code>: <code>loss = l1 * reduced_sum(abs(x))</code></li>
    <li><code>tf.keras.regularizers.l2(l1=0.01)</code>: <code>loss = l1 * reduced_sum(square(x))</code></li>
    <li><code>tf.keras.regularizers.l1_l2(l1=0.01, l2=0.02)</code></li>
</ul>

<p>For example:</p>
<pre class="rounded"><code class="python">layer = tf.keras.layers.Dense(
    units=64,
    kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),
    bias_regularizer=regularizers.l2(1e-4),
    activity_regularizer=regularizers.l2(1e-5)
)</code></pre>

            
        
            <!-- Subject header -->
            <h2 id="models">
                <a href="/learn/keras/models">Models</a>
            </h2>
            <p>A <a href="https://keras.io/api/models/model/" target="_blank">model</a> in keras is just a group of layers.</p>

<p>A complete example that goes through creating, training, and evaluating a keras model:</p>

<pre class="rounded"><code class="python">from sklearn.datasets import load_iris
from sklearn.preprocessing import LabelBinarizer
from sklearn.utils import shuffle
import keras

# Load dataset
iris = load_iris()
data = iris.data
enc = LabelBinarizer()
target = enc.fit_transform(iris.target)
X, y = shuffle(data, target, random_state=0)

# Make model
inputs = keras.Input(shape=(4,))
x = keras.layers.Dense(5, activation='relu')(inputs)
outputs = keras.layers.Dense(3, activation='softmax')(x)
model = keras.Model(inputs=inputs, outputs=outputs)

# Compile model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train
model.fit(x=X, y=y, batch_size=8, epochs=150, validation_split=0.3)

# Evaluate
loss, accuracy = model.evaluate(X, y)

# Predict
predictions = model.predict(X)  # softmax gives us a probability of each category
</code></pre>

            <!-- For each topic -->
            
                <!-- Topic header -->
                <h4 id="models-model">&bull; Model</h4>
                <!-- Include topic -->
                <p>The <code>Model</code> class requires two things, the <code>inputs</code> to a model and the <code>outputs</code> to a model. There is an optimal <code>name</code> parameter</p>

<p>There are two ways to instantiate a <code>Model</code> class:</p>
<ul>
    <li>Functions API</li>
    <li>By subclassing the <code>Model</code> class</li>
</ul>

<p>Function API method:</p>
<pre class="rounded"><code class="python">import tensorflow as tf
inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
</code></pre>

            
                <!-- Topic header -->
                <h4 id="models-save-and-load">&bull; Save And Load</h4>
                <!-- Include topic -->
                <h6>Save tf.keras</h6>

<p><a href="https://www.tensorflow.org/guide/keras/save_and_serialize" target="_blank">Saving</a> a model whole is super easy.</p>

<pre class="rounded"><code class="python">model.save('my_model')</code></pre>

<p>This will create a directory called <code>my_model</code> with <code>assets</code>, <code>saved_model.pb</code>, and <code>variables</code> as contents. This only works if using <code>tf.keras</code> instead of native keras. See below if using native keras.</p>

<p>To load it again:</p>
<pre class="rounded"><code class="python">model = tf.keras.models.load_model('my_model')</code></pre>

<h6>Save keras</h6>
<p><a href="https://keras.io/api/models/model_saving_apis/" target="_blank">Saving</a> the model as a single HDF5 is an option, however some items are not saved, such as custom layers and external losses and metrics. These can be quite annoying to add back later if you get a model from someone else.</p>

<p>H5:</p>
<pre class="rounded"><code class="python">model.save('model.h5')
model = tf.keras.models.load_model('model.h5')</code></pre>

<h6><a href="https://keras.io/api/models/model_saving_apis/" target="_blank">Other Save/Load Functions</a></h6>
<ul>
    <li><code>model.get_weights()</code></li>
    <li><code>model.set_weights(weights)</code></li>
    <li><code>model.save_weights('file_path.h5')</code></li>
    <li><code>model.load_weights('file_path.h5')</code></li>
    <li><code>model.to_json()</code></li>
    <li><code>model = tf.keras.models.model_from_json(config)</code></li>
    <li><code>new_model = tf.keras.models.clone_model(model)</code></li>
</ul>

            
                <!-- Topic header -->
                <h4 id="models-sequential">&bull; Sequential</h4>
                <!-- Include topic -->
                <p>The <a href="https://keras.io/api/models/sequential/" target="_blank">Sequential</a> class allows you to add layers sequentially to a model.</p>

<pre class="rounded"><code class="python">model = tf.keras.Sequential()
model.add(tf.keras.Input(shape=(16,)))  # Add input layer that accepts a feature vector of length 16
model.add(tf.keras.layers.Dense(6))  # Adds a layer containing 6 neurons</code></pre>

            
                <!-- Topic header -->
                <h4 id="models-summary">&bull; Summary</h4>
                <!-- Include topic -->
                <p><code>Model.summary()</code> can be used to summarize your model, but outputting the layers</p>

<pre class="rounded"><code>Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 3)]               0
_________________________________________________________________
dense (Dense)                (None, 4)                 16
_________________________________________________________________
dense_1 (Dense)              (None, 5)                 25
=================================================================
Total params: 41
Trainable params: 41
Non-trainable params: 0
_________________________________________________________________
</code></pre>

            
                <!-- Topic header -->
                <h4 id="models-training">&bull; Training</h4>
                <!-- Include topic -->
                <p>Keras <a href="https://keras.io/api/models/model_training_apis/" target="_blank">training</a> APIs involve compiling, fitting, evaluating, and predicting using a model.</p>

<h6>Compile</h6>

<p>Prepares the model for training (does a lot of hidden stuff).</p>

<pre class="rounded"><code class="python">Model.compile(
    optimizer="rmsprop",
    loss=None,
    metrics=None,
    loss_weights=None,
    weighted_metrics=None,
    run_eagerly=None,
    steps_per_execution=None,
    **kwargs
)</code></pre>

<ul>
    <li>Optimizer: <a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam" target="_blank">Adam</a> is the most popular optimizer.</li>
    <li>Loss: The neural network will try to minimize this value via the optimization algorithm. There are a large number of <a href="https://www.tensorflow.org/api_docs/python/tf/keras/losses" target="_blank">loss functions</a>. Most notably (note, for keras, you can use a string instead of the tensorflow functions by snake-casing the function as a string):</li>
    <ul>
        <li>Classification (Categorical) Data:</li>
        <ul>
            <li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy" target="_blank">BinaryCrossentropy</a>: Used when there are only two possible labels (0 and 1)</li>
            <li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy" target="_blank">CategoricalCrossentropy</a>: Used when there are two+ possible classes. Expects labels to be encoded via one-hot representation.</li>
            <li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy" target="_blank">SparseCategoricalCrossentropy</a>: Sibling to CategoricalCrossentropy. Expects an integer encoding instead of one-hot. Integers are distinct classes, similarity via closeness is not assumed.</li>
        </ul>
        <li>Regression (Continuous) Data:</li>
        <ul>
            <li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError" target="_blank">MeanSquaredError</a>: Loss = square(y_true - y_pred)</li>
        </ul>
    </ul>
    <li>Metrics: List of <a href="https://keras.io/api/metrics/" target="_blank">metrics</a> to output during training and returned during fitting. <code>['accuracy']</code> is the most common metric</li>
    <li>Loss Weights: If a list of losses is given as the lost function, you can specify how heavily waited each loss function is. For example <code>[10, 1]</code> would weight the first loss function 10 times heavier than the second loss function.</li>
</ul>

<h6>Fit</h6>

<p>Used to train a model.</p>

<pre class="rounded"><code class="python">Model.fit(
    x=None,
    y=None,
    batch_size=None,
    epochs=1,
    verbose=1,
    callbacks=None,
    validation_split=0.0,
    validation_data=None,
    shuffle=True,
    class_weight=None,
    sample_weight=None,
    initial_epoch=0,
    steps_per_epoch=None,
    validation_steps=None,
    validation_batch_size=None,
    validation_freq=1,
    max_queue_size=10,
    workers=1,
    use_multiprocessing=False,
)</code></pre>

<ul>
    <li>Verbose: 0 = silent, 1 = progress bar per epoch, 2 = one line output per epoch</li>
    <li>Callbacks: List of <a href="https://keras.io/api/callbacks/" target="_blank">callbacks</a>. My documentation on callbacks can be found <a href="/learn/keras/callbacks">here</a>.</li>
    <li>Validation Split: Float between 0 and 1. Fraction of training data to use for validation. Uses the ending % BEFORE shuffling.</li>
    <li>Validation Data: Data to use for validation. Data should be in <code>(x_val, y_val)</code> format. Do not use with <code>validation_split</code>.</li>
    <li>Class Weight: Dictionary mapping class indices (integers) to weight (float). Useful for unbalanced data (where there are more samples of one class than another)</li>
    <li>Sample Weight: Weigh samples differently. 1D numpy array of sample size is expected.</li>
    <li>Validation Frequency: How often, in terms of epochs, to validate the data.</li>
    <li>Generator Specific Arguments:</li>
    <ul>
        <li>Steps Per Epoch: Number of batches required to declare an epoch. Needed for generators. Same idea for validation_steps.</li>
        <li>Max Queue Size: Number of samples to queue for a generator. Defaults to 10.</li>
        <li>Workers: Number of workers used for generators. Defaults to 1.</li>
        <li>Use Multiprocessing: Use process-based threading for generators.</li>
    </ul>
</ul>

<p>Returns a <code>History</code> object that can be used for plotting.</p>

<h6>Evaluate</h6>

<p>Like fit, but without the training. Used to find the loss and metric values for the model.</p>

<pre class="rounded"><code class="python">Model.evaluate(
    x=None,
    y=None,
    batch_size=None,
    verbose=1,
    sample_weight=None,
    steps=None,
    callbacks=None,
    max_queue_size=10,
    workers=1,
    use_multiprocessing=False,
    return_dict=False,
)</code></pre>

<ul>
    <li>Return Dict: Return the loss and metric results as a dictionary instead of a list. Key is the name of the metric. If False, a list (or single value) is returned.</li>
</ul>

<h6>Predict</h6>

<p>Used to predict data. A batch is expected.</p>

<pre class="rounded"><code class="python">Model.predict(
    x,
    batch_size=None,
    verbose=0,
    steps=None,
    callbacks=None,
    max_queue_size=10,
    workers=1,
    use_multiprocessing=False,
)</code></pre>

<p>Numpy array of predictions is returned</p>

            
        
    </div>
</div>

        </div>
    </div>

    <!-- Footer -->
    <footer class="small text-center text-muted">
        <!-- Social Links -->
		<p>
            <a href="https://github.com/DaltonCole" target="_blank" class="fa fa-github" style="font-size:28px;color:black;"></a>
            &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="https://www.linkedin.com/in/daltoncole1/" target="_blank" class="fa fa-linkedin" style="font-size:28px;color:#2867B2;"></a>
        </p>

        <!-- Copyright -->
        
        <p class="copyright"> Copyright &copy; 2020 
            -2024 
         | Dalton Russell Cole</p>

        <!-- Extra Footer Block -->
        
        
    </footer>
</body>
</html>
